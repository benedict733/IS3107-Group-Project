{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightfm in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (1.17)\n",
      "Requirement already satisfied: numpy in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from lightfm) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from lightfm) (1.13.0)\n",
      "Requirement already satisfied: requests in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from lightfm) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from lightfm) (1.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from requests->lightfm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from requests->lightfm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from requests->lightfm) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from requests->lightfm) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from scikit-learn->lightfm) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages (from scikit-learn->lightfm) (3.4.0)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=2423b5c99addec4d303835aa3885298415038db9c6ad5144b520f07919c54955\n",
      "  Stored in directory: /Users/yaoyi/Library/Caches/pip/wheels/92/09/11/aa01d01a7f005fda8a66ad71d2be7f8aa341bddafb27eee3c7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightfm\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaoyi/opt/anaconda3/envs/airflow_env/lib/python3.9/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, auc_score, recall_at_k\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "import ast\n",
    "import pickle\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ET_steam_clean.csv')\n",
    "# Assuming `df` is your DataFrame with the structure provided.\n",
    "df_copy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.evaluation.python_evaluation import precision_at_k, recall_at_k, diversity, map_at_k, ndcg_at_k, auc\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k,\n",
    "                                                     recall_at_k, get_top_k_items,\n",
    "                                                     catalog_coverage, distributional_coverage, novelty, diversity, serendipity)\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType, StructType, StructField\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, CountVectorizer, VectorAssembler\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from recommenders.datasets.spark_splitters import spark_random_split\n",
    "from recommenders.datasets.python_splitters import python_chrono_split, python_stratified_split\n",
    "from recommenders.evaluation.spark_evaluation import SparkRankingEvaluation, SparkDiversityEvaluation\n",
    "from recommenders.utils.spark_utils import start_or_get_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'recommendationid', 'author', 'review',\n",
       "       'timestamp_created', 'timestamp_updated', 'voted_up', 'votes_up',\n",
       "       'weighted_vote_score', 'app_id', 'title', 'date_release', 'rating',\n",
       "       'positive_ratio', 'user_reviews', 'price_final', 'price_original', 'os',\n",
       "       'time_between_comment_and_game', 'days_between_comment_and_game',\n",
       "       'rating_numerical', 'rating_simplified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 417129 entries, 0 to 417128\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   userID  417129 non-null  int64\n",
      " 1   itemID  417129 non-null  int64\n",
      " 2   rating  417129 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 9.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_copy = df[['author', 'app_id', 'rating_numerical']]\n",
    "df_copy = df_copy.rename(columns={'author':'userID', 'app_id':'itemID', 'rating_numerical':'rating'})\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_stratified_split(\n",
    "    df_copy, ratio=0.8, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358973</th>\n",
       "      <td>76561197960269213</td>\n",
       "      <td>555260</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367218</th>\n",
       "      <td>76561197960269409</td>\n",
       "      <td>335920</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334063</th>\n",
       "      <td>76561197960270054</td>\n",
       "      <td>1816890</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199570</th>\n",
       "      <td>76561197960271994</td>\n",
       "      <td>1166780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289997</th>\n",
       "      <td>76561197960272599</td>\n",
       "      <td>447960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   userID   itemID  rating\n",
       "358973  76561197960269213   555260       0\n",
       "367218  76561197960269409   335920       3\n",
       "334063  76561197960270054  1816890       2\n",
       "199570  76561197960271994  1166780       0\n",
       "289997  76561197960272599   447960       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Filtering out users and items in the test set that do not appear in the training set.\n",
    "# This is done so that we can see if our model has learnt user's previous item interactions and can recommend relevant items.\n",
    "test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n",
    "test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]\n",
    "\n",
    "# Creating a test set which only contains the last interaction for each user. Remaining data of the user is used in the train set\n",
    "leave_one_out_test = test.groupby(\"userID\").last().reset_index()\n",
    "\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 16:34:32.998615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets.python_splitters import python_chrono_split, python_stratified_split\n",
    "\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "\n",
    "# Importing the NCF model class from the recommenders library\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "\n",
    "# importing the evaluation metrics\n",
    "from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k,\n",
    "                                                     recall_at_k, get_top_k_items,\n",
    "                                                     catalog_coverage, distributional_coverage, novelty, diversity, serendipity)\n",
    "from recommenders.utils.constants import SEED as DEFAULT_SEED\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Model parameters\n",
    "# Number of iterations during the training process\n",
    "EPOCHS = 5\n",
    "# Batch size means how many user-item pairs you want to predict at once\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Setting seed to remove any stochasticity and reproduce results\n",
    "SEED = DEFAULT_SEED  # Set None for non-deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing train.csv ...\n",
      "INFO:recommenders.models.ncf.dataset:Indexing leave_one_out_test.csv ...\n",
      "INFO:recommenders.models.ncf.dataset:Creating full leave-one-out test file leave_one_out_test_full.csv ...\n",
      "100%|██████████| 19894/19894 [06:46<00:00, 48.99it/s]\n",
      "INFO:recommenders.models.ncf.dataset:Indexing leave_one_out_test_full.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Writing the data into csv files\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "leave_one_out_test_file = \"leave_one_out_test.csv\"\n",
    "\n",
    "# train.to_csv(train_file, index=False)\n",
    "# test.to_csv(test_file, index=False)\n",
    "# leave_one_out_test.to_csv(leave_one_out_test_file, index=False)\n",
    "\n",
    "data = NCFDataset(train_file=train_file, test_file=leave_one_out_test_file, seed=SEED, overwrite_test_file_full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users,\n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=3,\n",
    "    layer_sizes=[64,24,8],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-2,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")\n",
    "from tqdm import tqdm\n",
    "with Timer() as train_time:\n",
    "    model.fit(data)\n",
    "\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time.interval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# https://github.com/recommenders-team/recommenders/issues/1735\n",
    "dir_path = 'NCF_model_trained'\n",
    "# # and then while loading depending on the type of your model in this case neumf pass it that dir parameter\n",
    "# model.load(neumf_dir='dir_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF(\n",
    "    n_users=data.n_users,\n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[64,32,16,8],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "dir_path = 'NCF_model_trained'\n",
    "model.load(neumf_dir = dir_path)\n",
    "\n",
    "model.user2id = data.user2id\n",
    "model.item2id = data.item2id\n",
    "model.id2user = data.id2user\n",
    "model.id2item = data.id2item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
